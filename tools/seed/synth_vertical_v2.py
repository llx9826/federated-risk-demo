#!/usr/bin/env python3
"""
çºµå‘è”é‚¦å­¦ä¹ åˆæˆæ•°æ®ç”Ÿæˆå™¨
æ”¯æŒå¤šæ–¹æ•°æ®ç”Ÿæˆï¼Œç¡®ä¿æœ‰æ•ˆä¿¡å·å’Œå¯å­¦ä¹ æ€§
"""

import argparse
import hashlib
import json
import os
import random
import sys
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any

import numpy as np
import pandas as pd
from scipy import stats
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import roc_auc_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler


class VerticalFLDataGenerator:
    """çºµå‘è”é‚¦å­¦ä¹ æ•°æ®ç”Ÿæˆå™¨"""
    
    def __init__(self, n_samples: int = 50000, overlap: float = 0.6, 
                 parties: List[str] = None, seed: int = 42, 
                 bad_rate: float = 0.12, noise: float = 0.15):
        """
        åˆå§‹åŒ–æ•°æ®ç”Ÿæˆå™¨
        
        Args:
            n_samples: æ¯æ–¹æ ·æœ¬æ•°é‡
            overlap: äº¤é›†æ¯”ä¾‹
            parties: å‚ä¸æ–¹åˆ—è¡¨ ['A', 'B', 'C']
            seed: éšæœºç§å­
            bad_rate: åè´¦ç‡
            noise: å™ªå£°æ°´å¹³
        """
        self.n_samples = n_samples
        self.overlap = overlap
        self.parties = parties or ['A', 'B']
        self.seed = seed
        self.bad_rate = bad_rate
        self.noise = noise
        self.public_salt = "federated_risk_demo_2025"
        
        # è®¾ç½®éšæœºç§å­
        np.random.seed(seed)
        random.seed(seed)
        
        # æ•°æ®è´¨é‡ç»Ÿè®¡
        self.data_profile = {
            'generation_time': datetime.now().isoformat(),
            'parameters': {
                'n_samples': n_samples,
                'overlap': overlap,
                'parties': parties,
                'seed': seed,
                'bad_rate': bad_rate,
                'noise': noise
            },
            'quality_metrics': {},
            'feature_correlations': {},
            'baseline_performance': {}
        }
    
    def generate_phone_numbers(self, n: int) -> List[str]:
        """ç”Ÿæˆæ‰‹æœºå·ç ä½œä¸ºPSIæ ‡è¯†ç¬¦"""
        phones = []
        for i in range(n):
            # ç”Ÿæˆä¸­å›½æ‰‹æœºå·æ ¼å¼
            prefix = random.choice(['138', '139', '150', '151', '188', '189'])
            suffix = f"{random.randint(10000000, 99999999):08d}"
            phones.append(f"{prefix}{suffix}")
        return phones
    
    def generate_psi_tokens(self, phones: List[str]) -> List[str]:
        """ç”ŸæˆPSIä»¤ç‰Œ"""
        tokens = []
        for phone in phones:
            token_input = f"{self.public_salt}{phone}"
            token = hashlib.sha256(token_input.encode()).hexdigest()
            tokens.append(token)
        return tokens
    
    def create_overlapping_identifiers(self) -> Tuple[List[str], Dict[str, List[str]]]:
        """åˆ›å»ºæœ‰é‡å çš„æ ‡è¯†ç¬¦"""
        # ç”Ÿæˆå…±åŒç”¨æˆ·æ± 
        overlap_size = int(self.n_samples * self.overlap)
        common_phones = self.generate_phone_numbers(overlap_size)
        
        party_phones = {}
        party_tokens = {}
        
        for party in self.parties:
            # æ¯æ–¹éƒ½åŒ…å«å…±åŒç”¨æˆ·
            party_specific_size = self.n_samples - overlap_size
            party_specific_phones = self.generate_phone_numbers(party_specific_size)
            
            # åˆå¹¶å…±åŒç”¨æˆ·å’Œä¸“æœ‰ç”¨æˆ·
            all_phones = common_phones + party_specific_phones
            random.shuffle(all_phones)
            
            party_phones[party] = all_phones
            party_tokens[party] = self.generate_psi_tokens(all_phones)
        
        return common_phones, party_tokens
    
    def generate_bank_features(self, n: int) -> pd.DataFrame:
        """ç”Ÿæˆé“¶è¡Œæ–¹ç‰¹å¾ï¼ˆAæ–¹ï¼Œå«æ ‡ç­¾ï¼‰"""
        data = {
            # æ”¶å…¥ç›¸å…³
            'annual_income': np.random.lognormal(10.5, 0.8, n),  # å¹´æ”¶å…¥
            'monthly_income': np.random.lognormal(8.5, 0.6, n),  # æœˆæ”¶å…¥
            
            # å€ºåŠ¡ç›¸å…³
            'debt_to_income': np.random.beta(2, 5, n) * 2,  # å€ºåŠ¡æ”¶å…¥æ¯”
            'cc_utilization': np.random.beta(2, 3, n),  # ä¿¡ç”¨å¡ä½¿ç”¨ç‡
            'total_debt': np.random.lognormal(9.0, 1.2, n),  # æ€»å€ºåŠ¡
            
            # ä¿¡ç”¨å†å²
            'credit_len_yrs': np.random.gamma(2, 5, n),  # ä¿¡ç”¨å†å²é•¿åº¦
            'credit_score': np.random.normal(650, 120, n),  # ä¿¡ç”¨è¯„åˆ†
            
            # è¿çº¦å†å²
            'late_3m': np.random.binomial(1, 0.15, n),  # è¿‘3æœˆé€¾æœŸ
            'delinq_12m': np.random.poisson(0.3, n),  # è¿‘12æœˆè¿çº¦æ¬¡æ•°
            'bankruptcy_flag': np.random.binomial(1, 0.02, n),  # ç ´äº§æ ‡è®°
            
            # è´¦æˆ·ä¿¡æ¯
            'num_accounts': np.random.poisson(3, n),  # è´¦æˆ·æ•°é‡
            'account_age_months': np.random.gamma(3, 12, n),  # è´¦æˆ·å¹´é¾„
            
            # å…¶ä»–
            'employment_years': np.random.gamma(2, 3, n),  # å·¥ä½œå¹´é™
            'home_ownership': np.random.choice(['own', 'rent', 'mortgage'], n, p=[0.3, 0.4, 0.3])
        }
        
        df = pd.DataFrame(data)
        
        # æ•°æ®æ¸…ç†å’Œçº¦æŸ
        df['annual_income'] = np.clip(df['annual_income'], 20000, 2000000)
        df['monthly_income'] = np.clip(df['monthly_income'], 2000, 200000)
        df['debt_to_income'] = np.clip(df['debt_to_income'], 0, 2)
        df['cc_utilization'] = np.clip(df['cc_utilization'], 0, 1)
        df['credit_score'] = np.clip(df['credit_score'], 300, 850)
        df['credit_len_yrs'] = np.clip(df['credit_len_yrs'], 0, 50)
        df['employment_years'] = np.clip(df['employment_years'], 0, 40)
        
        return df
    
    def generate_ecom_features(self, n: int) -> pd.DataFrame:
        """ç”Ÿæˆç”µå•†æ–¹ç‰¹å¾ï¼ˆBæ–¹ï¼‰"""
        data = {
            # è´­ä¹°è¡Œä¸º
            'order_cnt_6m': np.random.poisson(8, n),  # 6æœˆè®¢å•æ•°
            'monetary_6m': np.random.lognormal(6.5, 1.5, n),  # 6æœˆæ¶ˆè´¹é‡‘é¢
            'avg_order_value': np.random.lognormal(4.5, 0.8, n),  # å¹³å‡è®¢å•ä»·å€¼
            
            # é€€è´§è¡Œä¸º
            'return_rate': np.random.beta(1, 9, n),  # é€€è´§ç‡
            'return_cnt_6m': np.random.poisson(1.2, n),  # 6æœˆé€€è´§æ¬¡æ•°
            
            # æ´»è·ƒåº¦
            'recency_days': np.random.gamma(2, 15, n),  # æœ€è¿‘è´­ä¹°å¤©æ•°
            'frequency_score': np.random.gamma(2, 2, n),  # è´­ä¹°é¢‘ç‡è¯„åˆ†
            'session_duration_avg': np.random.lognormal(3.5, 0.8, n),  # å¹³å‡ä¼šè¯æ—¶é•¿
            
            # è¡Œä¸ºæ¨¡å¼
            'midnight_orders_ratio': np.random.beta(1, 19, n),  # æ·±å¤œä¸‹å•æ¯”ä¾‹
            'weekend_orders_ratio': np.random.beta(3, 7, n),  # å‘¨æœ«ä¸‹å•æ¯”ä¾‹
            'mobile_orders_ratio': np.random.beta(7, 3, n),  # ç§»åŠ¨ç«¯ä¸‹å•æ¯”ä¾‹
            
            # åå¥½
            'category_diversity': np.random.gamma(2, 1.5, n),  # å“ç±»å¤šæ ·æ€§
            'brand_loyalty_score': np.random.beta(3, 2, n),  # å“ç‰Œå¿ è¯šåº¦
            
            # é£é™©æŒ‡æ ‡
            'payment_failures': np.random.poisson(0.5, n),  # æ”¯ä»˜å¤±è´¥æ¬¡æ•°
            'address_changes': np.random.poisson(0.3, n),  # åœ°å€å˜æ›´æ¬¡æ•°
        }
        
        df = pd.DataFrame(data)
        
        # æ•°æ®æ¸…ç†å’Œçº¦æŸ
        df['monetary_6m'] = np.clip(df['monetary_6m'], 0, 100000)
        df['avg_order_value'] = np.clip(df['avg_order_value'], 10, 5000)
        df['recency_days'] = np.clip(df['recency_days'], 0, 365)
        df['session_duration_avg'] = np.clip(df['session_duration_avg'], 30, 7200)
        df['category_diversity'] = np.clip(df['category_diversity'], 1, 20)
        
        return df
    
    def generate_telco_features(self, n: int) -> pd.DataFrame:
        """ç”Ÿæˆè¿è¥å•†æ–¹ç‰¹å¾ï¼ˆCæ–¹ï¼Œå¯é€‰ï¼‰"""
        data = {
            # è´¦å•ä¿¡æ¯
            'monthly_bill': np.random.lognormal(4.0, 0.6, n),  # æœˆè´¦å•
            'bill_overdue_days': np.random.gamma(1, 3, n),  # è´¦å•é€¾æœŸå¤©æ•°
            'payment_method': np.random.choice(['auto', 'manual', 'prepaid'], n, p=[0.6, 0.3, 0.1]),
            
            # ä½¿ç”¨è¡Œä¸º
            'data_usage_gb': np.random.lognormal(2.5, 1.0, n),  # æ•°æ®ä½¿ç”¨é‡
            'call_minutes': np.random.lognormal(5.0, 0.8, n),  # é€šè¯æ—¶é•¿
            'sms_count': np.random.poisson(50, n),  # çŸ­ä¿¡æ•°é‡
            
            # æœåŠ¡ä¿¡æ¯
            'tenure_months': np.random.gamma(3, 8, n),  # åœ¨ç½‘æ—¶é•¿
            'plan_changes': np.random.poisson(0.5, n),  # å¥—é¤å˜æ›´æ¬¡æ•°
            'service_calls': np.random.poisson(1, n),  # å®¢æœé€šè¯æ¬¡æ•°
            
            # ç½‘ç»œè¡Œä¸º
            'roaming_usage': np.random.exponential(0.1, n),  # æ¼«æ¸¸ä½¿ç”¨
            'night_usage_ratio': np.random.beta(2, 8, n),  # å¤œé—´ä½¿ç”¨æ¯”ä¾‹
            'weekend_usage_ratio': np.random.beta(3, 7, n),  # å‘¨æœ«ä½¿ç”¨æ¯”ä¾‹
        }
        
        df = pd.DataFrame(data)
        
        # æ•°æ®æ¸…ç†å’Œçº¦æŸ
        df['monthly_bill'] = np.clip(df['monthly_bill'], 20, 500)
        df['bill_overdue_days'] = np.clip(df['bill_overdue_days'], 0, 90)
        df['data_usage_gb'] = np.clip(df['data_usage_gb'], 0, 100)
        df['call_minutes'] = np.clip(df['call_minutes'], 0, 5000)
        df['tenure_months'] = np.clip(df['tenure_months'], 1, 120)
        
        return df
    
    def generate_target_with_signal(self, bank_df: pd.DataFrame, 
                                   ecom_df: pd.DataFrame, 
                                   telco_df: pd.DataFrame = None) -> np.ndarray:
        """åŸºäºç‰¹å¾ç”Ÿæˆæœ‰ä¿¡å·çš„ç›®æ ‡å˜é‡"""
        n = len(bank_df)
        
        # é“¶è¡Œç‰¹å¾æƒé‡ï¼ˆæ­£ç›¸å…³è¡¨ç¤ºå¢åŠ è¿çº¦æ¦‚ç‡ï¼‰
        bank_weights = {
            'debt_to_income': 2.5,
            'cc_utilization': 1.8,
            'late_3m': 1.5,
            'delinq_12m': 1.2,
            'bankruptcy_flag': 3.0,
            'annual_income': -0.8,  # è´Ÿç›¸å…³
            'credit_len_yrs': -0.6,
            'credit_score': -1.5,
            'employment_years': -0.4
        }
        
        # ç”µå•†ç‰¹å¾æƒé‡
        ecom_weights = {
            'return_rate': 1.5,
            'recency_days': 0.8,
            'midnight_orders_ratio': 1.2,
            'payment_failures': 2.0,
            'order_cnt_6m': -0.6,  # è´Ÿç›¸å…³
            'monetary_6m': -0.9,
            'brand_loyalty_score': -0.5
        }
        
        # è¿è¥å•†ç‰¹å¾æƒé‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        telco_weights = {
            'bill_overdue_days': 1.8,
            'plan_changes': 0.8,
            'service_calls': 0.6,
            'tenure_months': -0.7  # è´Ÿç›¸å…³
        } if telco_df is not None else {}
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        scaler = StandardScaler()
        
        # è®¡ç®—é“¶è¡Œçº¿æ€§é¡¹
        bank_linear = np.zeros(n)
        for feature, weight in bank_weights.items():
            if feature in bank_df.columns:
                feature_std = scaler.fit_transform(bank_df[[feature]]).flatten()
                bank_linear += weight * feature_std
        
        # è®¡ç®—ç”µå•†çº¿æ€§é¡¹
        ecom_linear = np.zeros(n)
        for feature, weight in ecom_weights.items():
            if feature in ecom_df.columns:
                feature_std = scaler.fit_transform(ecom_df[[feature]]).flatten()
                ecom_linear += weight * feature_std
        
        # è®¡ç®—è¿è¥å•†çº¿æ€§é¡¹
        telco_linear = np.zeros(n)
        if telco_df is not None:
            for feature, weight in telco_weights.items():
                if feature in telco_df.columns:
                    feature_std = scaler.fit_transform(telco_df[[feature]]).flatten()
                    telco_linear += weight * feature_std
        
        # äº¤äº’é¡¹ï¼ˆAÃ—Bï¼‰
        interaction_terms = np.zeros(n)
        
        # å€ºåŠ¡æ”¶å…¥æ¯” Ã— é€€è´§ç‡
        if 'debt_to_income' in bank_df.columns and 'return_rate' in ecom_df.columns:
            dti_std = scaler.fit_transform(bank_df[['debt_to_income']]).flatten()
            return_std = scaler.fit_transform(ecom_df[['return_rate']]).flatten()
            interaction_terms += 1.5 * dti_std * return_std
        
        # ä¿¡ç”¨å¡ä½¿ç”¨ç‡ Ã— æœ€è¿‘è´­ä¹°å¤©æ•°
        if 'cc_utilization' in bank_df.columns and 'recency_days' in ecom_df.columns:
            cc_std = scaler.fit_transform(bank_df[['cc_utilization']]).flatten()
            recency_std = scaler.fit_transform(ecom_df[['recency_days']]).flatten()
            interaction_terms += 1.2 * cc_std * recency_std
        
        # é€¾æœŸè®°å½• Ã— æ”¯ä»˜å¤±è´¥
        if 'late_3m' in bank_df.columns and 'payment_failures' in ecom_df.columns:
            late_std = scaler.fit_transform(bank_df[['late_3m']]).flatten()
            payment_std = scaler.fit_transform(ecom_df[['payment_failures']]).flatten()
            interaction_terms += 2.0 * late_std * payment_std
        
        # åˆå¹¶æ‰€æœ‰é¡¹
        logits = bank_linear + ecom_linear + telco_linear + interaction_terms
        
        # æ·»åŠ å™ªå£°
        noise = np.random.normal(0, self.noise, n)
        logits += noise
        
        # è½¬æ¢ä¸ºæ¦‚ç‡
        probabilities = 1 / (1 + np.exp(-logits))
        
        # æ ¡å‡†åˆ°ç›®æ ‡åè´¦ç‡
        threshold = np.percentile(probabilities, (1 - self.bad_rate) * 100)
        labels = (probabilities > threshold).astype(int)
        
        # ç¡®ä¿åè´¦ç‡åœ¨åˆç†èŒƒå›´å†…
        actual_bad_rate = labels.mean()
        if not (0.08 <= actual_bad_rate <= 0.20):
            # é‡æ–°æ ¡å‡†
            sorted_probs = np.sort(probabilities)
            target_threshold_idx = int((1 - self.bad_rate) * len(sorted_probs))
            threshold = sorted_probs[target_threshold_idx]
            labels = (probabilities > threshold).astype(int)
        
        return labels
    
    def validate_data_quality(self, datasets: Dict[str, pd.DataFrame]) -> bool:
        """éªŒè¯æ•°æ®è´¨é‡"""
        print("\nğŸ” éªŒè¯æ•°æ®è´¨é‡...")
        
        all_passed = True
        
        for party, df in datasets.items():
            print(f"\nğŸ“Š éªŒè¯ {party} æ–¹æ•°æ®:")
            
            # æ£€æŸ¥æ ·æœ¬æ•°é‡
            if len(df) < self.n_samples * 0.95:
                print(f"âŒ æ ·æœ¬æ•°é‡ä¸è¶³: {len(df)} < {self.n_samples * 0.95}")
                all_passed = False
            else:
                print(f"âœ… æ ·æœ¬æ•°é‡: {len(df)}")
            
            # æ£€æŸ¥ç¼ºå¤±ç‡
            missing_rates = df.isnull().mean()
            high_missing = missing_rates[missing_rates > 0.4]
            if len(high_missing) > 0:
                print(f"âŒ é«˜ç¼ºå¤±ç‡ç‰¹å¾: {high_missing.to_dict()}")
                all_passed = False
            else:
                print(f"âœ… ç¼ºå¤±ç‡æ£€æŸ¥é€šè¿‡")
            
            # æ£€æŸ¥å¸¸é‡åˆ—
            constant_cols = []
            for col in df.select_dtypes(include=[np.number]).columns:
                if df[col].nunique() <= 1:
                    constant_cols.append(col)
            
            if constant_cols:
                print(f"âŒ å¸¸é‡åˆ—: {constant_cols}")
                all_passed = False
            else:
                print(f"âœ… æ— å¸¸é‡åˆ—")
            
            # æ£€æŸ¥é‡å¤æ ·æœ¬
            duplicate_rate = df.duplicated().mean()
            if duplicate_rate > 0.001:
                print(f"âŒ é‡å¤æ ·æœ¬ç‡è¿‡é«˜: {duplicate_rate:.4f}")
                all_passed = False
            else:
                print(f"âœ… é‡å¤æ ·æœ¬ç‡: {duplicate_rate:.4f}")
        
        return all_passed
    
    def calculate_feature_correlations(self, bank_df: pd.DataFrame, 
                                     ecom_df: pd.DataFrame, 
                                     labels: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—ç‰¹å¾ä¸æ ‡ç­¾çš„ç›¸å…³æ€§"""
        correlations = {}
        
        # é“¶è¡Œç‰¹å¾ç›¸å…³æ€§
        for col in bank_df.select_dtypes(include=[np.number]).columns:
            corr = np.corrcoef(bank_df[col], labels)[0, 1]
            if not np.isnan(corr):
                correlations[f'bank_{col}'] = abs(corr)
        
        # ç”µå•†ç‰¹å¾ç›¸å…³æ€§
        for col in ecom_df.select_dtypes(include=[np.number]).columns:
            corr = np.corrcoef(ecom_df[col], labels)[0, 1]
            if not np.isnan(corr):
                correlations[f'ecom_{col}'] = abs(corr)
        
        # æ£€æŸ¥æœ‰æ•ˆä¿¡å·æ•°é‡
        strong_signals = {k: v for k, v in correlations.items() if v >= 0.1}
        
        self.data_profile['feature_correlations'] = correlations
        self.data_profile['strong_signals_count'] = len(strong_signals)
        
        print(f"\nğŸ“ˆ å¼ºä¿¡å·ç‰¹å¾æ•°é‡: {len(strong_signals)}/6 (è¦æ±‚â‰¥6)")
        for feature, corr in sorted(strong_signals.items(), key=lambda x: x[1], reverse=True):
            print(f"  {feature}: {corr:.3f}")
        
        return correlations
    
    def test_baseline_performance(self, bank_df: pd.DataFrame, 
                                ecom_df: pd.DataFrame, 
                                labels: np.ndarray) -> Dict[str, float]:
        """æµ‹è¯•åŸºçº¿æ¨¡å‹æ€§èƒ½"""
        print("\nğŸ¯ æµ‹è¯•åŸºçº¿æ¨¡å‹æ€§èƒ½...")
        
        # åˆå¹¶ç‰¹å¾
        features = pd.concat([bank_df.select_dtypes(include=[np.number]), 
                            ecom_df.select_dtypes(include=[np.number])], axis=1)
        
        # å¤„ç†ç¼ºå¤±å€¼
        features = features.fillna(features.median())
        
        # åˆ†å‰²æ•°æ®
        X_train, X_test, y_train, y_test = train_test_split(
            features, labels, test_size=0.3, random_state=self.seed, stratify=labels
        )
        
        results = {}
        
        # é€»è¾‘å›å½’
        lr = LogisticRegression(random_state=self.seed, max_iter=1000)
        lr.fit(X_train, y_train)
        lr_pred = lr.predict_proba(X_test)[:, 1]
        lr_auc = roc_auc_score(y_test, lr_pred)
        
        # æ¢¯åº¦æå‡
        gb = GradientBoostingClassifier(random_state=self.seed, n_estimators=100)
        gb.fit(X_train, y_train)
        gb_pred = gb.predict_proba(X_test)[:, 1]
        gb_auc = roc_auc_score(y_test, gb_pred)
        
        # è®¡ç®—KSå€¼
        def calculate_ks(y_true, y_prob):
            from scipy.stats import ks_2samp
            pos_scores = y_prob[y_true == 1]
            neg_scores = y_prob[y_true == 0]
            return ks_2samp(pos_scores, neg_scores).statistic
        
        lr_ks = calculate_ks(y_test, lr_pred)
        gb_ks = calculate_ks(y_test, gb_pred)
        
        results = {
            'logistic_auc': lr_auc,
            'logistic_ks': lr_ks,
            'gradient_boosting_auc': gb_auc,
            'gradient_boosting_ks': gb_ks,
            'best_auc': max(lr_auc, gb_auc),
            'best_ks': max(lr_ks, gb_ks)
        }
        
        self.data_profile['baseline_performance'] = results
        
        print(f"ğŸ“Š é€»è¾‘å›å½’ - AUC: {lr_auc:.3f}, KS: {lr_ks:.3f}")
        print(f"ğŸ“Š æ¢¯åº¦æå‡ - AUC: {gb_auc:.3f}, KS: {gb_ks:.3f}")
        
        # æ£€æŸ¥æ€§èƒ½è¦æ±‚
        if results['best_auc'] < 0.70 or results['best_ks'] < 0.25:
            print(f"âŒ åŸºçº¿æ€§èƒ½ä¸è¾¾æ ‡ (AUC: {results['best_auc']:.3f} < 0.70 æˆ– KS: {results['best_ks']:.3f} < 0.25)")
            return None
        else:
            print(f"âœ… åŸºçº¿æ€§èƒ½è¾¾æ ‡")
            return results
    
    def generate_dataset_readme(self, datasets: Dict[str, pd.DataFrame], 
                              labels: np.ndarray) -> str:
        """ç”Ÿæˆæ•°æ®é›†è¯´æ˜æ–‡æ¡£"""
        readme_content = f"""# è”é‚¦å­¦ä¹ åˆæˆæ•°æ®é›†è¯´æ˜

## ç”Ÿæˆå‚æ•°
- æ ·æœ¬æ•°é‡: {self.n_samples:,}
- äº¤é›†æ¯”ä¾‹: {self.overlap:.1%}
- å‚ä¸æ–¹: {', '.join(self.parties)}
- éšæœºç§å­: {self.seed}
- åè´¦ç‡: {self.bad_rate:.1%}
- å™ªå£°æ°´å¹³: {self.noise}
- ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## æ•°æ®æ¦‚è§ˆ

### æ ‡ç­¾åˆ†å¸ƒ
- æ­£æ ·æœ¬ (è¿çº¦): {labels.sum():,} ({labels.mean():.1%})
- è´Ÿæ ·æœ¬ (æ­£å¸¸): {(1-labels).sum():,} ({(1-labels).mean():.1%})

### å„æ–¹ç‰¹å¾æ•°é‡
"""
        
        for party, df in datasets.items():
            party_name = {'A': 'é“¶è¡Œ', 'B': 'ç”µå•†', 'C': 'è¿è¥å•†'}.get(party, party)
            readme_content += f"- {party_name}æ–¹: {len(df.columns)} ä¸ªç‰¹å¾\n"
        
        readme_content += "\n## ç‰¹å¾è¯´æ˜\n\n"
        
        # é“¶è¡Œç‰¹å¾è¯´æ˜
        if 'A' in datasets:
            readme_content += """### é“¶è¡Œæ–¹ç‰¹å¾ (A)
- `annual_income`: å¹´æ”¶å…¥
- `monthly_income`: æœˆæ”¶å…¥  
- `debt_to_income`: å€ºåŠ¡æ”¶å…¥æ¯” (â†‘è¿çº¦é£é™©)
- `cc_utilization`: ä¿¡ç”¨å¡ä½¿ç”¨ç‡ (â†‘è¿çº¦é£é™©)
- `total_debt`: æ€»å€ºåŠ¡
- `credit_len_yrs`: ä¿¡ç”¨å†å²é•¿åº¦ (â†“è¿çº¦é£é™©)
- `credit_score`: ä¿¡ç”¨è¯„åˆ† (â†“è¿çº¦é£é™©)
- `late_3m`: è¿‘3æœˆé€¾æœŸæ ‡è®° (â†‘è¿çº¦é£é™©)
- `delinq_12m`: è¿‘12æœˆè¿çº¦æ¬¡æ•° (â†‘è¿çº¦é£é™©)
- `bankruptcy_flag`: ç ´äº§æ ‡è®°
- `num_accounts`: è´¦æˆ·æ•°é‡
- `account_age_months`: è´¦æˆ·å¹´é¾„
- `employment_years`: å·¥ä½œå¹´é™ (â†“è¿çº¦é£é™©)
- `home_ownership`: æˆ¿å±‹æ‰€æœ‰æƒ

"""
        
        # ç”µå•†ç‰¹å¾è¯´æ˜
        if 'B' in datasets:
            readme_content += """### ç”µå•†æ–¹ç‰¹å¾ (B)
- `order_cnt_6m`: 6æœˆè®¢å•æ•° (â†“è¿çº¦é£é™©)
- `monetary_6m`: 6æœˆæ¶ˆè´¹é‡‘é¢ (â†“è¿çº¦é£é™©)
- `avg_order_value`: å¹³å‡è®¢å•ä»·å€¼
- `return_rate`: é€€è´§ç‡ (â†‘è¿çº¦é£é™©)
- `return_cnt_6m`: 6æœˆé€€è´§æ¬¡æ•°
- `recency_days`: æœ€è¿‘è´­ä¹°å¤©æ•° (â†‘è¿çº¦é£é™©)
- `frequency_score`: è´­ä¹°é¢‘ç‡è¯„åˆ†
- `session_duration_avg`: å¹³å‡ä¼šè¯æ—¶é•¿
- `midnight_orders_ratio`: æ·±å¤œä¸‹å•æ¯”ä¾‹ (â†‘è¿çº¦é£é™©)
- `weekend_orders_ratio`: å‘¨æœ«ä¸‹å•æ¯”ä¾‹
- `mobile_orders_ratio`: ç§»åŠ¨ç«¯ä¸‹å•æ¯”ä¾‹
- `category_diversity`: å“ç±»å¤šæ ·æ€§
- `brand_loyalty_score`: å“ç‰Œå¿ è¯šåº¦ (â†“è¿çº¦é£é™©)
- `payment_failures`: æ”¯ä»˜å¤±è´¥æ¬¡æ•° (â†‘è¿çº¦é£é™©)
- `address_changes`: åœ°å€å˜æ›´æ¬¡æ•°

"""
        
        # è¿è¥å•†ç‰¹å¾è¯´æ˜
        if 'C' in datasets:
            readme_content += """### è¿è¥å•†æ–¹ç‰¹å¾ (C)
- `monthly_bill`: æœˆè´¦å•
- `bill_overdue_days`: è´¦å•é€¾æœŸå¤©æ•° (â†‘è¿çº¦é£é™©)
- `payment_method`: æ”¯ä»˜æ–¹å¼
- `data_usage_gb`: æ•°æ®ä½¿ç”¨é‡
- `call_minutes`: é€šè¯æ—¶é•¿
- `sms_count`: çŸ­ä¿¡æ•°é‡
- `tenure_months`: åœ¨ç½‘æ—¶é•¿ (â†“è¿çº¦é£é™©)
- `plan_changes`: å¥—é¤å˜æ›´æ¬¡æ•° (â†‘è¿çº¦é£é™©)
- `service_calls`: å®¢æœé€šè¯æ¬¡æ•°
- `roaming_usage`: æ¼«æ¸¸ä½¿ç”¨
- `night_usage_ratio`: å¤œé—´ä½¿ç”¨æ¯”ä¾‹
- `weekend_usage_ratio`: å‘¨æœ«ä½¿ç”¨æ¯”ä¾‹

"""
        
        readme_content += """## PSIæ ‡è¯†ç¬¦
- `psi_token`: SHA256(public_salt || phone) ç”¨äºéšç§æ±‚äº¤
- ä»…ç”¨äºæ•°æ®å¯¹é½ï¼Œä¸å‚ä¸ç‰¹å¾å·¥ç¨‹

## æ•°æ®è´¨é‡ä¿è¯
- æ‰€æœ‰ç‰¹å¾ç¼ºå¤±ç‡ < 40%
- æ— å…¨å¸¸é‡åˆ—
- é‡å¤æ ·æœ¬ < 0.1%
- æ—  NaN/Inf å€¼
- è‡³å°‘6ä¸ªç‰¹å¾ä¸æ ‡ç­¾ç›¸å…³æ€§ |Ï| â‰¥ 0.1

## åŸºçº¿æ€§èƒ½
- è¦æ±‚æ˜æ–‡åŸºçº¿ AUC â‰¥ 0.70, KS â‰¥ 0.25
- æ”¯æŒé€»è¾‘å›å½’å’Œæ¢¯åº¦æå‡æ¨¡å‹

## ä½¿ç”¨è¯´æ˜
```bash
# ç”Ÿæˆæ•°æ®
python tools/seed/synth_vertical_v2.py --n 50000 --overlap 0.6 --parties A,B --bad_rate 0.12 --noise 0.15

# éªŒè¯æ•°æ®åˆçº¦
python tools/contract/data_contract.py --files partyA_bank.csv partyB_ecom.csv
```
"""
        
        return readme_content
    
    def generate(self, output_dir: str = "data/synth") -> bool:
        """ç”Ÿæˆæ•°æ®é›†"""
        print(f"ğŸš€ å¼€å§‹ç”Ÿæˆçºµå‘è”é‚¦å­¦ä¹ æ•°æ®é›†...")
        print(f"ğŸ“Š å‚æ•°: n={self.n_samples}, overlap={self.overlap}, parties={self.parties}")
        print(f"ğŸ“Š åè´¦ç‡={self.bad_rate}, å™ªå£°={self.noise}, ç§å­={self.seed}")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(output_dir, exist_ok=True)
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                print(f"\nğŸ”„ å°è¯• {attempt + 1}/{max_retries}")
                
                # 1. ç”Ÿæˆé‡å æ ‡è¯†ç¬¦
                common_phones, party_tokens = self.create_overlapping_identifiers()
                
                # 2. ç”Ÿæˆå„æ–¹ç‰¹å¾
                datasets = {}
                
                if 'A' in self.parties:
                    bank_df = self.generate_bank_features(self.n_samples)
                    bank_df['psi_token'] = party_tokens['A']
                    datasets['A'] = bank_df
                
                if 'B' in self.parties:
                    ecom_df = self.generate_ecom_features(self.n_samples)
                    ecom_df['psi_token'] = party_tokens['B']
                    datasets['B'] = ecom_df
                
                if 'C' in self.parties:
                    telco_df = self.generate_telco_features(self.n_samples)
                    telco_df['psi_token'] = party_tokens['C']
                    datasets['C'] = telco_df
                
                # 3. ç”Ÿæˆæ ‡ç­¾ï¼ˆåŸºäºAæ–¹ï¼‰
                if 'A' in datasets:
                    labels = self.generate_target_with_signal(
                        datasets['A'], 
                        datasets.get('B', pd.DataFrame()), 
                        datasets.get('C')
                    )
                    datasets['A']['default_flag'] = labels
                
                # 4. éªŒè¯æ•°æ®è´¨é‡
                if not self.validate_data_quality(datasets):
                    print(f"âŒ æ•°æ®è´¨é‡éªŒè¯å¤±è´¥ï¼Œé‡æ–°ç”Ÿæˆ...")
                    continue
                
                # 5. è®¡ç®—ç‰¹å¾ç›¸å…³æ€§
                if 'A' in datasets and 'B' in datasets:
                    correlations = self.calculate_feature_correlations(
                        datasets['A'], datasets['B'], labels
                    )
                    
                    if self.data_profile['strong_signals_count'] < 6:
                        print(f"âŒ å¼ºä¿¡å·ç‰¹å¾ä¸è¶³ ({self.data_profile['strong_signals_count']}/6)ï¼Œé‡æ–°ç”Ÿæˆ...")
                        continue
                
                # 6. æµ‹è¯•åŸºçº¿æ€§èƒ½
                if 'A' in datasets and 'B' in datasets:
                    baseline_results = self.test_baseline_performance(
                        datasets['A'], datasets['B'], labels
                    )
                    
                    if baseline_results is None:
                        print(f"âŒ åŸºçº¿æ€§èƒ½ä¸è¾¾æ ‡ï¼Œé‡æ–°ç”Ÿæˆ...")
                        continue
                
                # 7. ä¿å­˜æ•°æ®é›†
                file_mapping = {
                    'A': 'partyA_bank.csv',
                    'B': 'partyB_ecom.csv', 
                    'C': 'partyC_telco.csv'
                }
                
                for party, df in datasets.items():
                    filename = file_mapping[party]
                    filepath = os.path.join(output_dir, filename)
                    df.to_csv(filepath, index=False)
                    print(f"ğŸ’¾ å·²ä¿å­˜: {filepath} ({len(df)} è¡Œ, {len(df.columns)} åˆ—)")
                
                # 8. ä¿å­˜æ•°æ®æ¦‚å†µ
                profile_path = os.path.join(output_dir, 'data_profile.json')
                with open(profile_path, 'w', encoding='utf-8') as f:
                    json.dump(self.data_profile, f, indent=2, ensure_ascii=False)
                print(f"ğŸ“Š æ•°æ®æ¦‚å†µå·²ä¿å­˜: {profile_path}")
                
                # 9. ç”ŸæˆREADME
                readme_content = self.generate_dataset_readme(datasets, labels)
                readme_path = os.path.join(output_dir, 'DATASET_README.md')
                with open(readme_path, 'w', encoding='utf-8') as f:
                    f.write(readme_content)
                print(f"ğŸ“– æ•°æ®è¯´æ˜å·²ä¿å­˜: {readme_path}")
                
                print(f"\nâœ… æ•°æ®ç”ŸæˆæˆåŠŸ!")
                print(f"ğŸ“Š äº¤é›†å¤§å°: {len(common_phones):,} ({len(common_phones)/self.n_samples:.1%})")
                print(f"ğŸ“Š åè´¦ç‡: {labels.mean():.1%}")
                print(f"ğŸ“Š åŸºçº¿AUC: {baseline_results['best_auc']:.3f}")
                print(f"ğŸ“Š åŸºçº¿KS: {baseline_results['best_ks']:.3f}")
                
                return True
                
            except Exception as e:
                print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
                if attempt == max_retries - 1:
                    print(f"\nğŸ’¡ ä¿®å¤å»ºè®®:")
                    print(f"1. æ£€æŸ¥éšæœºç§å­è®¾ç½®")
                    print(f"2. è°ƒæ•´åè´¦ç‡å‚æ•° (0.08-0.20)")
                    print(f"3. å¢åŠ å™ªå£°æ°´å¹³")
                    print(f"4. æ£€æŸ¥ç‰¹å¾æƒé‡è®¾ç½®")
                    return False
                continue
        
        return False


def main():
    parser = argparse.ArgumentParser(description='çºµå‘è”é‚¦å­¦ä¹ åˆæˆæ•°æ®ç”Ÿæˆå™¨')
    parser.add_argument('--n', type=int, default=50000, help='æ¯æ–¹æ ·æœ¬æ•°é‡')
    parser.add_argument('--overlap', type=float, default=0.6, help='äº¤é›†æ¯”ä¾‹')
    parser.add_argument('--parties', type=str, default='A,B', help='å‚ä¸æ–¹åˆ—è¡¨ï¼Œé€—å·åˆ†éš”')
    parser.add_argument('--seed', type=int, default=42, help='éšæœºç§å­')
    parser.add_argument('--bad_rate', type=float, default=0.12, help='åè´¦ç‡')
    parser.add_argument('--noise', type=float, default=0.15, help='å™ªå£°æ°´å¹³')
    parser.add_argument('--output', type=str, default='data/synth', help='è¾“å‡ºç›®å½•')
    
    args = parser.parse_args()
    
    # è§£æå‚ä¸æ–¹
    parties = [p.strip().upper() for p in args.parties.split(',')]
    
    # éªŒè¯å‚æ•°
    if not (0.08 <= args.bad_rate <= 0.20):
        print(f"âŒ åè´¦ç‡å¿…é¡»åœ¨ 0.08-0.20 ä¹‹é—´ï¼Œå½“å‰: {args.bad_rate}")
        sys.exit(1)
    
    if not (0.4 <= args.overlap <= 0.8):
        print(f"âŒ äº¤é›†æ¯”ä¾‹å¿…é¡»åœ¨ 0.4-0.8 ä¹‹é—´ï¼Œå½“å‰: {args.overlap}")
        sys.exit(1)
    
    if args.n < 10000:
        print(f"âŒ æ ·æœ¬æ•°é‡ä¸èƒ½å°‘äº 10000ï¼Œå½“å‰: {args.n}")
        sys.exit(1)
    
    # ç”Ÿæˆæ•°æ®
    generator = VerticalFLDataGenerator(
        n_samples=args.n,
        overlap=args.overlap,
        parties=parties,
        seed=args.seed,
        bad_rate=args.bad_rate,
        noise=args.noise
    )
    
    success = generator.generate(args.output)
    
    if success:
        print(f"\nğŸ‰ æ•°æ®ç”Ÿæˆå®Œæˆ! è¾“å‡ºç›®å½•: {args.output}")
        sys.exit(0)
    else:
        print(f"\nâŒ æ•°æ®ç”Ÿæˆå¤±è´¥!")
        sys.exit(1)


if __name__ == '__main__':
    main()