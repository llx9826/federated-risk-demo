#!/usr/bin/env python3
"""
äº§å‡ºç‰©å®Œæ•´æ€§éªŒè¯å·¥å…·

éªŒè¯æ‰€æœ‰åŸºå‡†æµ‹è¯•äº§å‡ºç‰©çš„å®Œæ•´æ€§å’Œæ•°æ®å¯è¿½æº¯æ€§
"""

import json
import hashlib
import os
from pathlib import Path
from typing import Dict, List, Tuple

import pandas as pd
from loguru import logger

class DeliverableValidator:
    """äº§å‡ºç‰©éªŒè¯å™¨"""
    
    def __init__(self, project_root: str = "."):
        """åˆå§‹åŒ–éªŒè¯å™¨"""
        self.project_root = Path(project_root)
        self.reports_dir = self.project_root / "reports"
        self.docs_dir = self.project_root / "docs"
        self.assets_dir = self.docs_dir / "assets"
        self.models_dir = self.project_root / "models"
        
        logger.info(f"åˆå§‹åŒ–äº§å‡ºç‰©éªŒè¯å™¨: {self.project_root}")
    
    def validate_file_integrity(self, file_path: Path) -> Dict:
        """éªŒè¯æ–‡ä»¶å®Œæ•´æ€§"""
        if not file_path.exists():
            return {
                'exists': False,
                'size': 0,
                'sha256': None,
                'readable': False
            }
        
        try:
            with open(file_path, 'rb') as f:
                content = f.read()
                sha256_hash = hashlib.sha256(content).hexdigest()
            
            return {
                'exists': True,
                'size': len(content),
                'sha256': sha256_hash,
                'readable': True
            }
        except Exception as e:
            return {
                'exists': True,
                'size': file_path.stat().st_size if file_path.exists() else 0,
                'sha256': None,
                'readable': False,
                'error': str(e)
            }
    
    def validate_psi_deliverables(self) -> Dict:
        """éªŒè¯PSIç›¸å…³äº§å‡ºç‰©"""
        logger.info("éªŒè¯PSIäº§å‡ºç‰©...")
        
        results = {
            'psi_summary': None,
            'psi_results': None,
            'psi_integrity': None,
            'psi_charts': []
        }
        
        # éªŒè¯PSIæ±‡æ€»æ•°æ®
        psi_summary_file = self.reports_dir / "psi_summary.csv"
        results['psi_summary'] = self.validate_file_integrity(psi_summary_file)
        
        if results['psi_summary']['exists']:
            try:
                df = pd.read_csv(psi_summary_file)
                results['psi_summary']['records'] = len(df)
                results['psi_summary']['columns'] = list(df.columns)
                logger.info(f"âœ“ PSIæ±‡æ€»æ•°æ®: {len(df)} æ¡è®°å½•")
            except Exception as e:
                results['psi_summary']['parse_error'] = str(e)
                logger.error(f"âœ— PSIæ±‡æ€»æ•°æ®è§£æå¤±è´¥: {e}")
        
        # éªŒè¯PSIè¯¦ç»†ç»“æœ
        psi_results_file = self.reports_dir / "psi_results.jsonl"
        results['psi_results'] = self.validate_file_integrity(psi_results_file)
        
        if results['psi_results']['exists']:
            try:
                with open(psi_results_file, 'r') as f:
                    lines = [line.strip() for line in f if line.strip()]
                    results['psi_results']['records'] = len(lines)
                logger.info(f"âœ“ PSIè¯¦ç»†ç»“æœ: {len(lines)} æ¡è®°å½•")
            except Exception as e:
                results['psi_results']['parse_error'] = str(e)
                logger.error(f"âœ— PSIè¯¦ç»†ç»“æœè§£æå¤±è´¥: {e}")
        
        # éªŒè¯PSIå®Œæ•´æ€§æ•°æ®
        psi_integrity_file = self.reports_dir / "psi_integrity.json"
        results['psi_integrity'] = self.validate_file_integrity(psi_integrity_file)
        
        if results['psi_integrity']['exists']:
            try:
                with open(psi_integrity_file, 'r') as f:
                    integrity_data = json.load(f)
                    results['psi_integrity']['test_id'] = integrity_data.get('test_id')
                    results['psi_integrity']['accuracy'] = integrity_data.get('accuracy')
                logger.info(f"âœ“ PSIå®Œæ•´æ€§æ•°æ®: ç²¾åº¦ = {integrity_data.get('accuracy', 'N/A')}")
            except Exception as e:
                results['psi_integrity']['parse_error'] = str(e)
                logger.error(f"âœ— PSIå®Œæ•´æ€§æ•°æ®è§£æå¤±è´¥: {e}")
        
        # éªŒè¯PSIå›¾è¡¨
        psi_charts = ['psi_throughput.png', 'psi_latency.png']
        for chart in psi_charts:
            chart_file = self.assets_dir / chart
            chart_info = self.validate_file_integrity(chart_file)
            chart_info['name'] = chart
            results['psi_charts'].append(chart_info)
            
            if chart_info['exists']:
                logger.info(f"âœ“ PSIå›¾è¡¨: {chart}")
            else:
                logger.warning(f"âœ— PSIå›¾è¡¨ç¼ºå¤±: {chart}")
        
        return results
    
    def validate_train_deliverables(self) -> Dict:
        """éªŒè¯è®­ç»ƒç›¸å…³äº§å‡ºç‰©"""
        logger.info("éªŒè¯è®­ç»ƒäº§å‡ºç‰©...")
        
        results = {
            'train_summary': None,
            'train_reports': [],
            'train_models': [],
            'train_charts': []
        }
        
        # éªŒè¯è®­ç»ƒæ±‡æ€»æ•°æ®
        train_summary_file = self.project_root / "bench" / "train-bench" / "data" / "train_results" / "train_summary.csv"
        results['train_summary'] = self.validate_file_integrity(train_summary_file)
        
        if results['train_summary']['exists']:
            try:
                df = pd.read_csv(train_summary_file)
                results['train_summary']['records'] = len(df)
                results['train_summary']['columns'] = list(df.columns)
                logger.info(f"âœ“ è®­ç»ƒæ±‡æ€»æ•°æ®: {len(df)} æ¡è®°å½•")
            except Exception as e:
                results['train_summary']['parse_error'] = str(e)
                logger.error(f"âœ— è®­ç»ƒæ±‡æ€»æ•°æ®è§£æå¤±è´¥: {e}")
        
        # éªŒè¯è®­ç»ƒæŠ¥å‘Šæ–‡ä»¶
        for report_file in self.reports_dir.glob("train_report_*.json"):
            report_info = self.validate_file_integrity(report_file)
            report_info['name'] = report_file.name
            
            if report_info['exists']:
                try:
                    with open(report_file, 'r') as f:
                        report_data = json.load(f)
                        report_info['test_id'] = report_data.get('test_id')
                        report_info['final_auc'] = report_data.get('final_auc')
                        report_info['final_ks'] = report_data.get('final_ks')
                    logger.info(f"âœ“ è®­ç»ƒæŠ¥å‘Š: {report_file.name}")
                except Exception as e:
                    report_info['parse_error'] = str(e)
                    logger.error(f"âœ— è®­ç»ƒæŠ¥å‘Šè§£æå¤±è´¥: {report_file.name}")
            
            results['train_reports'].append(report_info)
        
        # éªŒè¯æ¨¡å‹æ–‡ä»¶
        for model_file in self.models_dir.glob("*.pkl"):
            model_info = self.validate_file_integrity(model_file)
            model_info['name'] = model_file.name
            results['train_models'].append(model_info)
            
            if model_info['exists']:
                logger.info(f"âœ“ æ¨¡å‹æ–‡ä»¶: {model_file.name}")
            else:
                logger.warning(f"âœ— æ¨¡å‹æ–‡ä»¶ç¼ºå¤±: {model_file.name}")
        
        # éªŒè¯è®­ç»ƒå›¾è¡¨
        train_charts = ['train_auc_ks_vs_epsilon.png', 'train_comm_vs_round.png']
        for chart in train_charts:
            chart_file = self.assets_dir / chart
            chart_info = self.validate_file_integrity(chart_file)
            chart_info['name'] = chart
            results['train_charts'].append(chart_info)
            
            if chart_info['exists']:
                logger.info(f"âœ“ è®­ç»ƒå›¾è¡¨: {chart}")
            else:
                logger.warning(f"âœ— è®­ç»ƒå›¾è¡¨ç¼ºå¤±: {chart}")
        
        return results
    
    def validate_documentation(self) -> Dict:
        """éªŒè¯æ–‡æ¡£äº§å‡ºç‰©"""
        logger.info("éªŒè¯æ–‡æ¡£äº§å‡ºç‰©...")
        
        results = {
            'showcase_doc': None,
            'other_docs': []
        }
        
        # éªŒè¯å±•ç¤ºæ–‡æ¡£
        showcase_file = self.docs_dir / "pab-federated-showcase.md"
        results['showcase_doc'] = self.validate_file_integrity(showcase_file)
        
        if results['showcase_doc']['exists']:
            try:
                with open(showcase_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    results['showcase_doc']['word_count'] = len(content.split())
                    results['showcase_doc']['line_count'] = len(content.split('\n'))
                logger.info(f"âœ“ å±•ç¤ºæ–‡æ¡£: {results['showcase_doc']['word_count']} è¯")
            except Exception as e:
                results['showcase_doc']['parse_error'] = str(e)
                logger.error(f"âœ— å±•ç¤ºæ–‡æ¡£è§£æå¤±è´¥: {e}")
        
        # éªŒè¯å…¶ä»–æ–‡æ¡£
        doc_files = ['README.md', 'ARCHITECTURE.md', 'SECURITY.md', 'COMPLIANCE.md']
        for doc_file in doc_files:
            doc_path = self.docs_dir / doc_file
            doc_info = self.validate_file_integrity(doc_path)
            doc_info['name'] = doc_file
            results['other_docs'].append(doc_info)
            
            if doc_info['exists']:
                logger.info(f"âœ“ æ–‡æ¡£: {doc_file}")
            else:
                logger.warning(f"âœ— æ–‡æ¡£ç¼ºå¤±: {doc_file}")
        
        return results
    
    def validate_traceability(self) -> Dict:
        """éªŒè¯æ•°æ®å¯è¿½æº¯æ€§"""
        logger.info("éªŒè¯æ•°æ®å¯è¿½æº¯æ€§...")
        
        results = {
            'file_manifest': {},
            'hash_consistency': True,
            'timestamp_consistency': True,
            'id_consistency': True
        }
        
        # ç”Ÿæˆæ–‡ä»¶æ¸…å•
        for file_path in self.reports_dir.rglob("*"):
            if file_path.is_file() and file_path.suffix in ['.csv', '.jsonl', '.json']:
                try:
                    with open(file_path, 'rb') as f:
                        content = f.read()
                        sha256_hash = hashlib.sha256(content).hexdigest()
                        relative_path = file_path.relative_to(self.project_root)
                        results['file_manifest'][str(relative_path)] = {
                            'sha256': sha256_hash,
                            'size': len(content),
                            'modified': file_path.stat().st_mtime
                        }
                except Exception as e:
                    logger.warning(f"æ— æ³•å¤„ç†æ–‡ä»¶ {file_path}: {e}")
        
        logger.info(f"âœ“ æ–‡ä»¶æ¸…å•: {len(results['file_manifest'])} ä¸ªæ–‡ä»¶")
        
        # éªŒè¯æµ‹è¯•IDä¸€è‡´æ€§
        test_ids = set()
        
        # ä»PSIæ±‡æ€»ä¸­æå–æµ‹è¯•ID
        psi_summary_file = self.reports_dir / "psi_summary.csv"
        if psi_summary_file.exists():
            try:
                df = pd.read_csv(psi_summary_file)
                for test_id in df['test_id']:
                    test_ids.add(test_id)
            except Exception as e:
                logger.warning(f"æ— æ³•ä»PSIæ±‡æ€»ä¸­æå–æµ‹è¯•ID: {e}")
        
        # ä»è®­ç»ƒæ±‡æ€»ä¸­æå–æµ‹è¯•ID
        train_summary_file = self.project_root / "bench" / "train-bench" / "data" / "train_results" / "train_summary.csv"
        if train_summary_file.exists():
            try:
                df = pd.read_csv(train_summary_file)
                for test_id in df['test_id']:
                    test_ids.add(test_id)
            except Exception as e:
                logger.warning(f"æ— æ³•ä»è®­ç»ƒæ±‡æ€»ä¸­æå–æµ‹è¯•ID: {e}")
        
        results['unique_test_ids'] = len(test_ids)
        logger.info(f"âœ“ å”¯ä¸€æµ‹è¯•ID: {len(test_ids)} ä¸ª")
        
        return results
    
    def generate_validation_report(self) -> Dict:
        """ç”Ÿæˆå®Œæ•´éªŒè¯æŠ¥å‘Š"""
        logger.info("ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
        
        report = {
            'timestamp': pd.Timestamp.now().isoformat(),
            'project_root': str(self.project_root),
            'psi_deliverables': self.validate_psi_deliverables(),
            'train_deliverables': self.validate_train_deliverables(),
            'documentation': self.validate_documentation(),
            'traceability': self.validate_traceability()
        }
        
        # è®¡ç®—æ€»ä½“è¯„åˆ†
        total_files = 0
        valid_files = 0
        
        # PSIæ–‡ä»¶
        for key in ['psi_summary', 'psi_results', 'psi_integrity']:
            if report['psi_deliverables'][key]:
                total_files += 1
                if report['psi_deliverables'][key]['exists']:
                    valid_files += 1
        
        for chart in report['psi_deliverables']['psi_charts']:
            total_files += 1
            if chart['exists']:
                valid_files += 1
        
        # è®­ç»ƒæ–‡ä»¶
        if report['train_deliverables']['train_summary']:
            total_files += 1
            if report['train_deliverables']['train_summary']['exists']:
                valid_files += 1
        
        total_files += len(report['train_deliverables']['train_reports'])
        valid_files += sum(1 for r in report['train_deliverables']['train_reports'] if r['exists'])
        
        total_files += len(report['train_deliverables']['train_models'])
        valid_files += sum(1 for m in report['train_deliverables']['train_models'] if m['exists'])
        
        for chart in report['train_deliverables']['train_charts']:
            total_files += 1
            if chart['exists']:
                valid_files += 1
        
        # æ–‡æ¡£æ–‡ä»¶
        if report['documentation']['showcase_doc']:
            total_files += 1
            if report['documentation']['showcase_doc']['exists']:
                valid_files += 1
        
        total_files += len(report['documentation']['other_docs'])
        valid_files += sum(1 for d in report['documentation']['other_docs'] if d['exists'])
        
        report['summary'] = {
            'total_files': total_files,
            'valid_files': valid_files,
            'completion_rate': valid_files / total_files if total_files > 0 else 0,
            'file_manifest_count': len(report['traceability']['file_manifest']),
            'unique_test_ids': report['traceability']['unique_test_ids']
        }
        
        logger.info(f"âœ… éªŒè¯å®Œæˆ: {valid_files}/{total_files} æ–‡ä»¶æœ‰æ•ˆ ({report['summary']['completion_rate']:.1%})")
        
        return report
    
    def save_validation_report(self, report: Dict) -> str:
        """ä¿å­˜éªŒè¯æŠ¥å‘Š"""
        output_file = self.reports_dir / "validation_report.json"
        
        try:
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, indent=2, ensure_ascii=False)
            
            logger.info(f"éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
            return str(output_file)
            
        except Exception as e:
            logger.error(f"ä¿å­˜éªŒè¯æŠ¥å‘Šå¤±è´¥: {e}")
            raise

def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="éªŒè¯åŸºå‡†æµ‹è¯•äº§å‡ºç‰©å®Œæ•´æ€§")
    parser.add_argument("--project-root", default=".",
                       help="é¡¹ç›®æ ¹ç›®å½• (é»˜è®¤: å½“å‰ç›®å½•)")
    parser.add_argument("--save-report", action="store_true",
                       help="ä¿å­˜éªŒè¯æŠ¥å‘Šåˆ°æ–‡ä»¶")
    
    args = parser.parse_args()
    
    # åˆ›å»ºéªŒè¯å™¨
    validator = DeliverableValidator(args.project_root)
    
    try:
        # ç”ŸæˆéªŒè¯æŠ¥å‘Š
        report = validator.generate_validation_report()
        
        # ä¿å­˜æŠ¥å‘Š
        if args.save_report:
            output_file = validator.save_validation_report(report)
            print(f"\nğŸ“„ éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        
        # æ‰“å°æ‘˜è¦
        summary = report['summary']
        print(f"\nğŸ“Š éªŒè¯æ‘˜è¦:")
        print(f"  - æ€»æ–‡ä»¶æ•°: {summary['total_files']}")
        print(f"  - æœ‰æ•ˆæ–‡ä»¶æ•°: {summary['valid_files']}")
        print(f"  - å®Œæˆç‡: {summary['completion_rate']:.1%}")
        print(f"  - æ–‡ä»¶æ¸…å•: {summary['file_manifest_count']} ä¸ªæ–‡ä»¶")
        print(f"  - å”¯ä¸€æµ‹è¯•ID: {summary['unique_test_ids']} ä¸ª")
        
        if summary['completion_rate'] >= 0.9:
            print(f"\nâœ… äº§å‡ºç‰©éªŒè¯é€šè¿‡! å®Œæˆç‡ {summary['completion_rate']:.1%}")
            return 0
        else:
            print(f"\nâš ï¸  äº§å‡ºç‰©éªŒè¯è­¦å‘Š! å®Œæˆç‡ {summary['completion_rate']:.1%} < 90%")
            return 1
        
    except Exception as e:
        logger.error(f"éªŒè¯å¤±è´¥: {e}")
        return 1

if __name__ == '__main__':
    exit(main())